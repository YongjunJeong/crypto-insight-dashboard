{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "226320c0-746b-4d91-b786-320c563befd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Python Notebook: Bronze ingest from Binance Kline REST\n",
    "import time, json, random, datetime as dt\n",
    "import requests\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# =========================\n",
    "# (A) 실행 설정\n",
    "# =========================\n",
    "MODE           = \"backfill\"                   # once | poll | forever | backfill\n",
    "SYMBOLS        = [\"BTCUSDT\",\"ETHUSDT\"]      # 원하는 코인 심볼들\n",
    "INTERVALS      = [\"15m\"]                      # [\"1m\",\"5m\"] 등\n",
    "LIMIT_ONCE     = 1000                           # 호출당 캔들 수(최대 1000)\n",
    "POLL_SECONDS   = 10                             # poll/forever 간격(초)\n",
    "MAX_POLLS      = 60                             # poll 모드 반복 횟수\n",
    "BACKFILL_HOURS = 168                            # 과거 7일 = 168시간\n",
    "\n",
    "# 업서트 시 ingest_time 갱신 정책:\n",
    "# - True  : 매 업서트 때 ingest_time을 최신값으로 갱신\n",
    "# - False : 최초 ingest_time을 보존(이력 보존 성격 강화)\n",
    "UPSERT_UPDATE_INGEST_TIME = True\n",
    "\n",
    "# =========================\n",
    "# (B) 프로젝트 설정\n",
    "# =========================\n",
    "CATALOG = \"demo_catalog\"\n",
    "SCHEMA  = \"demo_schema\"\n",
    "TABLE        = f\"{CATALOG}.{SCHEMA}.bronze_charts\"       # Bronze Delta\n",
    "STATE_TABLE  = f\"{CATALOG}.{SCHEMA}.bronze_ingest_state\" # 마지막 open_time(ms) 상태 저장\n",
    "\n",
    "# Binance REST (Spot)\n",
    "BASE_URL = \"https://api.binance.com\"\n",
    "KLINES   = \"/api/v3/klines\"\n",
    "LIMIT_DEFAULT = 500  # Spot 기본 limit (최대 1000)\n",
    "\n",
    "# =========================\n",
    "# (C) 테이블 준비\n",
    "# =========================\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "spark.sql(f\"CREATE SCHEMA  IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "\n",
    "# Bronze 테이블: dt 파티션(날짜) + unique_key 인덱싱 성격\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {TABLE} (\n",
    "  source            STRING,\n",
    "  event_time        TIMESTAMP,  -- open_time(UTC)\n",
    "  ingest_time       TIMESTAMP,  -- 적재 시각(UTC)\n",
    "  unique_key        STRING,     -- \"symbol|interval|open_time(ms)\"\n",
    "  raw_json          STRING,     -- 원본 배열 JSON 문자열\n",
    "  api_endpoint      STRING,     -- 호출한 REST endpoint\n",
    "  api_params_hash   STRING,     -- 요청 파라미터 해시(디버깅/재현)\n",
    "  dt                DATE        -- event_date(partition key)\n",
    ") USING DELTA\n",
    "PARTITIONED BY (dt)\n",
    "\"\"\")\n",
    "\n",
    "# 상태 테이블: 심볼/인터벌별 마지막 open_time(ms)\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {STATE_TABLE} (\n",
    "  symbol    STRING,\n",
    "  interval  STRING,\n",
    "  last_open_time_ms LONG,\n",
    "  updated_at TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# =========================\n",
    "# (D) 유틸리티\n",
    "# =========================\n",
    "def _params_hash(params: Dict) -> str:\n",
    "    \"\"\"요청 파라미터 dict -> 안정적인 해시 문자열\"\"\"\n",
    "    raw = json.dumps(params, sort_keys=True, separators=(\",\",\":\"))\n",
    "    import hashlib\n",
    "    return hashlib.sha256(raw.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def _to_ms(ts: dt.datetime) -> int:\n",
    "    \"\"\"UTC-aware datetime -> epoch milliseconds\"\"\"\n",
    "    if ts.tzinfo is None:\n",
    "        ts = ts.replace(tzinfo=dt.timezone.utc)\n",
    "    return int(ts.timestamp() * 1000)\n",
    "\n",
    "def _from_ms(ms: int) -> dt.datetime:\n",
    "    \"\"\"epoch milliseconds -> UTC-aware datetime\"\"\"\n",
    "    return dt.datetime.fromtimestamp(ms/1000, tz=dt.timezone.utc)\n",
    "\n",
    "def binance_klines(symbol: str, interval: str, start_ms: Optional[int]=None,\n",
    "                   end_ms: Optional[int]=None, limit: int=LIMIT_DEFAULT,\n",
    "                   max_retries: int=5) -> Tuple[List[list], Dict[str,str]]:\n",
    "    \"\"\"\n",
    "    Binance Spot /api/v3/klines 호출\n",
    "    - 429(rate limit) 시 지수 백오프\n",
    "    - 성공 시 (rows, headers) 반환\n",
    "    \"\"\"\n",
    "    url = BASE_URL + KLINES\n",
    "    q = {\"symbol\": symbol, \"interval\": interval, \"limit\": limit}\n",
    "    if start_ms is not None: q[\"startTime\"] = start_ms\n",
    "    if end_ms   is not None: q[\"endTime\"]   = end_ms\n",
    "\n",
    "    delay = 1\n",
    "    last_headers = {}\n",
    "    for _ in range(max_retries):\n",
    "        r = requests.get(url, params=q, timeout=30)\n",
    "        last_headers = {k: v for k, v in r.headers.items()}\n",
    "        if r.status_code == 200:\n",
    "            return r.json(), last_headers\n",
    "        if r.status_code == 429:\n",
    "            time.sleep(delay + random.uniform(0, 0.3))  # 지터 포함\n",
    "            delay = min(delay * 2, 16)\n",
    "            continue\n",
    "        time.sleep(1 + random.uniform(0, 0.3))          # 기타 오류 재시도\n",
    "    r.raise_for_status()\n",
    "    return [], last_headers\n",
    "\n",
    "def _get_last_state(symbol: str, interval: str) -> Optional[int]:\n",
    "    \"\"\"STATE_TABLE에서 마지막 open_time(ms) 조회\"\"\"\n",
    "    df = spark.sql(f\"\"\"\n",
    "      SELECT last_open_time_ms\n",
    "      FROM {STATE_TABLE}\n",
    "      WHERE symbol = '{symbol}' AND interval = '{interval}'\n",
    "      ORDER BY updated_at DESC\n",
    "      LIMIT 1\n",
    "    \"\"\")\n",
    "    rows = df.collect()\n",
    "    return rows[0][0] if rows else None\n",
    "\n",
    "def _upsert_state(symbol: str, interval: str, last_ms: int):\n",
    "    \"\"\"STATE_TABLE에 마지막 open_time(ms) UPSERT\"\"\"\n",
    "    now = dt.datetime.now(dt.timezone.utc).isoformat()\n",
    "    spark.sql(f\"\"\"\n",
    "      MERGE INTO {STATE_TABLE} t\n",
    "      USING (SELECT '{symbol}' AS symbol, '{interval}' AS interval,\n",
    "                    {last_ms} AS last_open_time_ms, TIMESTAMP('{now}') AS updated_at) s\n",
    "      ON t.symbol = s.symbol AND t.interval = s.interval\n",
    "      WHEN MATCHED THEN UPDATE SET last_open_time_ms = s.last_open_time_ms, updated_at = s.updated_at\n",
    "      WHEN NOT MATCHED THEN INSERT (symbol, interval, last_open_time_ms, updated_at)\n",
    "      VALUES (s.symbol, s.interval, s.last_open_time_ms, s.updated_at)\n",
    "    \"\"\")\n",
    "\n",
    "# ===================================\n",
    "# (E) Bronze 테이블 데이터 채우기(중복 방지)\n",
    "# ===================================\n",
    "def _rows_to_bronze(symbol: str, interval: str, rows: List[list], endpoint: str, params: Dict):\n",
    "    \"\"\"\n",
    "    Binance klines rows(list of lists) -> Bronze Delta UPSERT\n",
    "    - 배치 내 dropDuplicates(unique_key)\n",
    "    - MERGE ON (unique_key AND dt) 로 idempotent 보장 + 파티션 프루닝\n",
    "    \"\"\"\n",
    "    if not rows:\n",
    "        return 0, None\n",
    "\n",
    "    param_hash = _params_hash(params)\n",
    "    now = dt.datetime.now(dt.timezone.utc)\n",
    "    recs = []\n",
    "    max_open_ms = None\n",
    "\n",
    "    # Binance kline item spec:\n",
    "    # [0] open_time(ms), [1] open, [2] high, [3] low, [4] close, [5] volume, [6] close_time(ms), ...\n",
    "    for item in rows:\n",
    "        open_ms = int(item[0])\n",
    "        event_time = _from_ms(open_ms)\n",
    "        unique = f\"{symbol}|{interval}|{open_ms}\"\n",
    "        recs.append({\n",
    "          \"source\": \"binance.spot.klines\",\n",
    "          \"event_time\": event_time.isoformat(),       # 문자열로 넣고 아래에서 TIMESTAMP 캐스팅\n",
    "          \"ingest_time\": now.isoformat(),\n",
    "          \"unique_key\": unique,\n",
    "          \"raw_json\": json.dumps(item, separators=(\",\",\":\")),\n",
    "          \"api_endpoint\": endpoint,\n",
    "          \"api_params_hash\": param_hash,\n",
    "          \"dt\": event_time.date().isoformat()         # 파티션 키\n",
    "        })\n",
    "        if (max_open_ms is None) or (open_ms > max_open_ms):\n",
    "            max_open_ms = open_ms\n",
    "\n",
    "    # 소스 DF 생성(+타입 캐스팅) & 배치 내 중복 제거\n",
    "    schema = StructType([\n",
    "        StructField(\"source\",           StringType(), True),\n",
    "        StructField(\"event_time\",       StringType(), True),\n",
    "        StructField(\"ingest_time\",      StringType(), True),\n",
    "        StructField(\"unique_key\",       StringType(), True),\n",
    "        StructField(\"raw_json\",         StringType(), True),\n",
    "        StructField(\"api_endpoint\",     StringType(), True),\n",
    "        StructField(\"api_params_hash\",  StringType(), True),\n",
    "        StructField(\"dt\",               StringType(), True),\n",
    "    ])\n",
    "    df = spark.createDataFrame([Row(**r) for r in recs], schema) \\\n",
    "        .withColumn(\"event_time\",  to_timestamp(col(\"event_time\"))) \\\n",
    "        .withColumn(\"ingest_time\", to_timestamp(col(\"ingest_time\"))) \\\n",
    "        .withColumn(\"dt\",          col(\"dt\").cast(\"date\")) \\\n",
    "        .dropDuplicates([\"unique_key\"])\n",
    "\n",
    "    # Delta MERGE: (unique_key AND dt) 기준으로 idempotent UPSERT\n",
    "    delta_table = DeltaTable.forName(spark, TABLE)\n",
    "\n",
    "    # ingest_time 갱신 정책에 따른 SET 구성\n",
    "    if UPSERT_UPDATE_INGEST_TIME:\n",
    "        set_map = {\n",
    "            \"source\":       \"s.source\",\n",
    "            \"event_time\":   \"s.event_time\",\n",
    "            \"ingest_time\":  \"s.ingest_time\",   # 매 업서트마다 최신으로 갱신\n",
    "            \"raw_json\":     \"s.raw_json\",\n",
    "            \"api_endpoint\": \"s.api_endpoint\",\n",
    "            \"api_params_hash\":\"s.api_params_hash\",\n",
    "            \"dt\":           \"s.dt\"\n",
    "        }\n",
    "    else:\n",
    "        set_map = {\n",
    "            \"source\":       \"s.source\",\n",
    "            \"event_time\":   \"s.event_time\",\n",
    "            \"ingest_time\":  \"t.ingest_time\",   # 최초값 보존\n",
    "            \"raw_json\":     \"s.raw_json\",\n",
    "            \"api_endpoint\": \"s.api_endpoint\",\n",
    "            \"api_params_hash\":\"s.api_params_hash\",\n",
    "            \"dt\":           \"s.dt\"\n",
    "        }\n",
    "\n",
    "    (delta_table.alias(\"t\")\n",
    "        .merge(df.alias(\"s\"), \"t.unique_key = s.unique_key AND t.dt = s.dt\")  # 파티션 포함 조인\n",
    "        .whenMatchedUpdate(set=set_map)\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute())\n",
    "\n",
    "    return df.count(), max_open_ms\n",
    "\n",
    "# =========================\n",
    "# (F) 모드별 동작\n",
    "# =========================\n",
    "def backfill_symbol(symbol: str, interval: str,\n",
    "                    hours: int = BACKFILL_HOURS, limit: int = LIMIT_DEFAULT):\n",
    "    \"\"\"\n",
    "    과거 구간을 윈도우로 끊어 안전하게 백필\n",
    "    - STATE_TABLE의 last_open_time_ms가 있으면 그 다음부터 이어 받기\n",
    "    - 6시간 창(batch_end) 단위로 요청해 rate limit/오류에 안전\n",
    "    \"\"\"\n",
    "    now_utc = dt.datetime.now(dt.timezone.utc)\n",
    "    start_utc = now_utc - dt.timedelta(hours=hours)\n",
    "\n",
    "    last_ms = _get_last_state(symbol, interval)\n",
    "    if last_ms:\n",
    "        start_utc = _from_ms(last_ms) + dt.timedelta(milliseconds=1)\n",
    "\n",
    "    start_ms = _to_ms(start_utc)\n",
    "    end_ms   = _to_ms(now_utc)\n",
    "\n",
    "    total_rows = 0\n",
    "    cursor_ms = start_ms\n",
    "\n",
    "    print(f\"[BACKFILL] {symbol} {interval} {_from_ms(start_ms)} → {_from_ms(end_ms)}\")\n",
    "    while cursor_ms < end_ms:\n",
    "        batch_end = min(end_ms, cursor_ms + 1000 * 60 * 60 * 6)  # 6시간 창\n",
    "        params = {\"symbol\": symbol, \"interval\": interval,\n",
    "                  \"startTime\": cursor_ms, \"endTime\": batch_end, \"limit\": limit}\n",
    "        rows, headers = binance_klines(symbol, interval,\n",
    "                                       start_ms=cursor_ms, end_ms=batch_end, limit=limit)\n",
    "        count, max_open_ms = _rows_to_bronze(symbol, interval, rows, KLINES, params)\n",
    "\n",
    "        used_weight = headers.get(\"X-MBX-USED-WEIGHT-1m\") or headers.get(\"X-MBX-USED-WEIGHT\")\n",
    "        if used_weight:\n",
    "            print(f\"  used_weight(1m): {used_weight}\")\n",
    "\n",
    "        total_rows += count\n",
    "        if max_open_ms is None:\n",
    "            # 더 이상 데이터가 없을 때, 다음 창으로 넘어감\n",
    "            cursor_ms = batch_end + 1\n",
    "        else:\n",
    "            # 마지막 open_time 이후부터 이어서 수집\n",
    "            cursor_ms = max_open_ms + 1\n",
    "            _upsert_state(symbol, interval, max_open_ms)\n",
    "\n",
    "        time.sleep(0.2)  # rate 완화\n",
    "\n",
    "    print(f\"[BACKFILL DONE] {symbol} {interval}: {total_rows} rows\")\n",
    "\n",
    "def poll_once(symbol: str, interval: str, limit: int = 50):\n",
    "    \"\"\"\n",
    "    근실시간 폴링(1회)\n",
    "    - limit(20~100 권장)으로 최근 캔들 묶음 수집\n",
    "    \"\"\"\n",
    "    params = {\"symbol\": symbol, \"interval\": interval, \"limit\": limit}\n",
    "    rows, headers = binance_klines(symbol, interval, limit=limit)\n",
    "    count, max_open_ms = _rows_to_bronze(symbol, interval, rows, KLINES, params)\n",
    "    if max_open_ms is not None:\n",
    "        _upsert_state(symbol, interval, max_open_ms)\n",
    "    used_weight = headers.get(\"X-MBX-USED-WEIGHT-1m\") or headers.get(\"X-MBX-USED-WEIGHT\")\n",
    "    print(f\"[POLL] {symbol} {interval}: +{count} rows, last_open_ms={max_open_ms}, used_weight={used_weight}\")\n",
    "\n",
    "# =========================\n",
    "# (G) MAIN\n",
    "# =========================\n",
    "if MODE == \"backfill\":\n",
    "    for iv in INTERVALS:\n",
    "        for sym in SYMBOLS:\n",
    "            backfill_symbol(sym, iv, hours=BACKFILL_HOURS, limit=LIMIT_DEFAULT)\n",
    "    dbutils.notebook.exit(\"backfill done\")\n",
    "\n",
    "elif MODE == \"poll\":\n",
    "    for i in range(MAX_POLLS):\n",
    "        for iv in INTERVALS:\n",
    "            for sym in SYMBOLS:\n",
    "                poll_once(sym, iv, limit=LIMIT_ONCE)\n",
    "        time.sleep(POLL_SECONDS)\n",
    "    dbutils.notebook.exit(\"poll done\")\n",
    "\n",
    "elif MODE == \"forever\":\n",
    "    print(f\"[LIVE] start polling every {POLL_SECONDS}s\")\n",
    "    while True:\n",
    "        try:\n",
    "            for iv in INTERVALS:\n",
    "                for sym in SYMBOLS:\n",
    "                    poll_once(sym, iv, limit=LIMIT_ONCE)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] {e}\")\n",
    "            time.sleep(5)\n",
    "        time.sleep(POLL_SECONDS)\n",
    "\n",
    "else:  # MODE == \"once\"\n",
    "    for iv in INTERVALS:\n",
    "        for sym in SYMBOLS:\n",
    "            poll_once(sym, iv, limit=LIMIT_ONCE)\n",
    "    dbutils.notebook.exit(\"once done\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4738524208626266,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingest_bronze_binance.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
