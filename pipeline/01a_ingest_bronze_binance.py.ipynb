{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "226320c0-746b-4d91-b786-320c563befd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Python Notebook: Bronze ingest from Binance Kline REST (append-only)\n",
    "import time, json, random, datetime as dt\n",
    "import requests\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# ===== (0) 권장 클러스터 설정 =====\n",
    "spark.conf.set(\"spark.databricks.delta.optimizeWrite\",\"true\")\n",
    "spark.conf.set(\"spark.databricks.delta.autoCompact\",\"true\")\n",
    "\n",
    "# =========================\n",
    "# (A) 실행 설정\n",
    "# =========================\n",
    "MODE           = \"backfill\"                     # once | poll | forever | backfill\n",
    "# SYMBOLS        = [\"BTCUSDT\",\"ETHUSDT\"]\n",
    "SYMBOLS        = [\"BTCUSDT\"]\n",
    "INTERVALS      = [\"15m\"]\n",
    "LIMIT_ONCE     = 1000\n",
    "POLL_SECONDS   = 10\n",
    "MAX_POLLS      = 60\n",
    "BACKFILL_HOURS = 168\n",
    "\n",
    "# =========================\n",
    "# (B) 프로젝트 설정\n",
    "# =========================\n",
    "CATALOG = \"demo_catalog\"\n",
    "SCHEMA  = \"demo_schema\"\n",
    "TABLE        = f\"{CATALOG}.{SCHEMA}.bronze_charts\"       # Bronze Delta (append-only)\n",
    "STATE_TABLE  = f\"{CATALOG}.{SCHEMA}.bronze_ingest_state\" # 마지막 open_time(ms) 상태 저장\n",
    "\n",
    "# Binance REST (Spot)\n",
    "BASE_URL = \"https://api.binance.com\"\n",
    "KLINES   = \"/api/v3/klines\"\n",
    "LIMIT_DEFAULT = 500  # Spot 기본 limit (최대 1000)\n",
    "\n",
    "# =========================\n",
    "# (C) 테이블 준비\n",
    "# =========================\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "spark.sql(f\"CREATE SCHEMA  IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {TABLE} (\n",
    "  source            STRING,\n",
    "  event_time        TIMESTAMP,  -- open_time(UTC)\n",
    "  ingest_time       TIMESTAMP,  -- 적재 시각(UTC)\n",
    "  unique_key        STRING,     -- \"symbol|interval|open_time(ms)\"\n",
    "  raw_json          STRING,     -- 원본 배열 JSON 문자열\n",
    "  api_endpoint      STRING,     -- 호출한 REST endpoint\n",
    "  api_params_hash   STRING,     -- 요청 파라미터 해시(디버깅/재현)\n",
    "  dt                DATE        -- event_date(partition key)\n",
    ") USING DELTA\n",
    "PARTITIONED BY (dt)\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {STATE_TABLE} (\n",
    "  symbol     STRING,\n",
    "  interval   STRING,\n",
    "  last_open_time_ms LONG,\n",
    "  updated_at TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# =========================\n",
    "# (D) 유틸리티\n",
    "# =========================\n",
    "def _params_hash(params: Dict) -> str:\n",
    "    raw = json.dumps(params, sort_keys=True, separators=(\",\",\":\"))\n",
    "    import hashlib\n",
    "    return hashlib.sha256(raw.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def _to_ms(ts: dt.datetime) -> int:\n",
    "    if ts.tzinfo is None:\n",
    "        ts = ts.replace(tzinfo=dt.timezone.utc)\n",
    "    return int(ts.timestamp() * 1000)\n",
    "\n",
    "def _from_ms(ms: int) -> dt.datetime:\n",
    "    return dt.datetime.fromtimestamp(ms/1000, tz=dt.timezone.utc)\n",
    "\n",
    "def binance_klines(symbol: str, interval: str, start_ms: Optional[int]=None,\n",
    "                   end_ms: Optional[int]=None, limit: int=LIMIT_DEFAULT,\n",
    "                   max_retries: int=5) -> Tuple[List[list], Dict[str,str]]:\n",
    "    url = BASE_URL + KLINES\n",
    "    q = {\"symbol\": symbol, \"interval\": interval, \"limit\": limit}\n",
    "    if start_ms is not None: q[\"startTime\"] = start_ms\n",
    "    if end_ms   is not None: q[\"endTime\"]   = end_ms\n",
    "\n",
    "    delay = 1\n",
    "    last_headers = {}\n",
    "    for _ in range(max_retries):\n",
    "        r = requests.get(url, params=q, timeout=30)\n",
    "        last_headers = {k: v for k, v in r.headers.items()}\n",
    "        if r.status_code == 200:\n",
    "            return r.json(), last_headers\n",
    "        if r.status_code == 429:\n",
    "            time.sleep(delay + random.uniform(0, 0.3))\n",
    "            delay = min(delay * 2, 16)\n",
    "            continue\n",
    "        time.sleep(1 + random.uniform(0, 0.3))\n",
    "    r.raise_for_status()\n",
    "    return [], last_headers\n",
    "\n",
    "def _get_last_state(symbol: str, interval: str) -> Optional[int]:\n",
    "    df = spark.sql(f\"\"\"\n",
    "      SELECT last_open_time_ms\n",
    "      FROM {STATE_TABLE}\n",
    "      WHERE symbol = '{symbol}' AND interval = '{interval}'\n",
    "      ORDER BY updated_at DESC\n",
    "      LIMIT 1\n",
    "    \"\"\")\n",
    "    rows = df.collect()\n",
    "    return rows[0][0] if rows else None\n",
    "\n",
    "def _upsert_state(symbol: str, interval: str, last_ms: int):\n",
    "    # 오프셋 없는 안전한 TIMESTAMP 문자열\n",
    "    now = dt.datetime.now(dt.timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    spark.sql(f\"\"\"\n",
    "      MERGE INTO {STATE_TABLE} t\n",
    "      USING (SELECT '{symbol}' AS symbol, '{interval}' AS interval,\n",
    "                    {last_ms} AS last_open_time_ms, TIMESTAMP('{now}') AS updated_at) s\n",
    "      ON t.symbol = s.symbol AND t.interval = s.interval\n",
    "      WHEN MATCHED THEN UPDATE SET last_open_time_ms = s.last_open_time_ms, updated_at = s.updated_at\n",
    "      WHEN NOT MATCHED THEN INSERT (symbol, interval, last_open_time_ms, updated_at)\n",
    "      VALUES (s.symbol, s.interval, s.last_open_time_ms, s.updated_at)\n",
    "    \"\"\")\n",
    "\n",
    "# ===================================\n",
    "# (E) Bronze append-only 작성\n",
    "# ===================================\n",
    "def _append_to_bronze(symbol: str, interval: str, rows: List[list], endpoint: str, params: Dict):\n",
    "    if not rows:\n",
    "        return 0, None\n",
    "\n",
    "    param_hash = _params_hash(params)\n",
    "    now = dt.datetime.now(dt.timezone.utc)\n",
    "    now_s = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    recs = []\n",
    "    max_open_ms = None\n",
    "\n",
    "    for item in rows:\n",
    "        open_ms = int(item[0])\n",
    "        event_time = _from_ms(open_ms)\n",
    "        unique = f\"{symbol}|{interval}|{open_ms}\"\n",
    "        recs.append({\n",
    "          \"source\": \"binance.spot.klines\",\n",
    "          \"event_time\": event_time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "          \"ingest_time\": now_s,\n",
    "          \"unique_key\": unique,\n",
    "          \"raw_json\": json.dumps(item, separators=(\",\",\":\")),\n",
    "          \"api_endpoint\": endpoint,\n",
    "          \"api_params_hash\": param_hash,\n",
    "          \"dt\": event_time.date().isoformat()\n",
    "        })\n",
    "        if (max_open_ms is None) or (open_ms > max_open_ms):\n",
    "            max_open_ms = open_ms\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"source\",           StringType(), True),\n",
    "        StructField(\"event_time\",       StringType(), True),\n",
    "        StructField(\"ingest_time\",      StringType(), True),\n",
    "        StructField(\"unique_key\",       StringType(), True),\n",
    "        StructField(\"raw_json\",         StringType(), True),\n",
    "        StructField(\"api_endpoint\",     StringType(), True),\n",
    "        StructField(\"api_params_hash\",  StringType(), True),\n",
    "        StructField(\"dt\",               StringType(), True),\n",
    "    ])\n",
    "    df = (spark.createDataFrame([Row(**r) for r in recs], schema)\n",
    "            .withColumn(\"event_time\",  to_timestamp(col(\"event_time\")))\n",
    "            .withColumn(\"ingest_time\", to_timestamp(col(\"ingest_time\")))\n",
    "            .withColumn(\"dt\",          col(\"dt\").cast(\"date\"))\n",
    "            .dropDuplicates([\"unique_key\"])     # 배치 내부만 중복 제거\n",
    "            .repartition(\"dt\"))                  # 파티션별 파일 수 최소화\n",
    "\n",
    "    count = df.count()  # 액션 1회(정확한 적재 건수 보고용)\n",
    "    df.writeTo(TABLE).append()\n",
    "    return count, max_open_ms\n",
    "\n",
    "# =========================\n",
    "# (F) 모드별 동작\n",
    "# =========================\n",
    "def backfill_symbol(symbol: str, interval: str,\n",
    "                    hours: int = BACKFILL_HOURS, limit: int = LIMIT_DEFAULT):\n",
    "    now_utc = dt.datetime.now(dt.timezone.utc)\n",
    "    start_utc = now_utc - dt.timedelta(hours=hours)\n",
    "\n",
    "    last_ms = _get_last_state(symbol, interval)\n",
    "    if last_ms:\n",
    "        start_utc = _from_ms(last_ms) + dt.timedelta(milliseconds=1)\n",
    "\n",
    "    start_ms = _to_ms(start_utc)\n",
    "    end_ms   = _to_ms(now_utc)\n",
    "\n",
    "    total_rows = 0\n",
    "    cursor_ms = start_ms\n",
    "\n",
    "    print(f\"[BACKFILL] {symbol} {interval} {_from_ms(start_ms)} → {_from_ms(end_ms)}\")\n",
    "    while cursor_ms < end_ms:\n",
    "        batch_end = min(end_ms, cursor_ms + 1000 * 60 * 60 * 6)  # 6시간 창\n",
    "        params = {\"symbol\": symbol, \"interval\": interval,\n",
    "                  \"startTime\": cursor_ms, \"endTime\": batch_end, \"limit\": limit}\n",
    "        rows, headers = binance_klines(symbol, interval,\n",
    "                                       start_ms=cursor_ms, end_ms=batch_end, limit=limit)\n",
    "        count, max_open_ms = _append_to_bronze(symbol, interval, rows, KLINES, params)\n",
    "\n",
    "        used_weight = headers.get(\"X-MBX-USED-WEIGHT-1m\") or headers.get(\"X-MBX-USED-WEIGHT\")\n",
    "        if used_weight:\n",
    "            print(f\"  used_weight(1m): {used_weight}\")\n",
    "\n",
    "        total_rows += count\n",
    "        if max_open_ms is None:\n",
    "            cursor_ms = batch_end + 1\n",
    "        else:\n",
    "            cursor_ms = max_open_ms + 1\n",
    "            _upsert_state(symbol, interval, max_open_ms)\n",
    "\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    print(f\"[BACKFILL DONE] {symbol} {interval}: {total_rows} rows\")\n",
    "\n",
    "def poll_once(symbol: str, interval: str, limit: int = 50):\n",
    "    params = {\"symbol\": symbol, \"interval\": interval, \"limit\": limit}\n",
    "    rows, headers = binance_klines(symbol, interval, limit=limit)\n",
    "    count, max_open_ms = _append_to_bronze(symbol, interval, rows, KLINES, params)\n",
    "    if max_open_ms is not None:\n",
    "        _upsert_state(symbol, interval, max_open_ms)\n",
    "    used_weight = headers.get(\"X-MBX-USED-WEIGHT-1m\") or headers.get(\"X-MBX-USED-WEIGHT\")\n",
    "    print(f\"[POLL] {symbol} {interval}: +{count} rows, last_open_ms={max_open_ms}, used_weight={used_weight}\")\n",
    "\n",
    "# =========================\n",
    "# (G) MAIN\n",
    "# =========================\n",
    "if MODE == \"backfill\":\n",
    "    for iv in INTERVALS:\n",
    "        for sym in SYMBOLS:\n",
    "            backfill_symbol(sym, iv, hours=BACKFILL_HOURS, limit=LIMIT_DEFAULT)\n",
    "    dbutils.notebook.exit(\"backfill done\")\n",
    "\n",
    "elif MODE == \"poll\":\n",
    "    for i in range(MAX_POLLS):\n",
    "        for iv in INTERVALS:\n",
    "            for sym in SYMBOLS:\n",
    "                poll_once(sym, iv, limit=LIMIT_ONCE)\n",
    "        time.sleep(POLL_SECONDS)\n",
    "    dbutils.notebook.exit(\"poll done\")\n",
    "\n",
    "elif MODE == \"forever\":\n",
    "    print(f\"[LIVE] start polling every {POLL_SECONDS}s\")\n",
    "    while True:\n",
    "        try:\n",
    "            for iv in INTERVALS:\n",
    "                for sym in SYMBOLS:\n",
    "                    poll_once(sym, iv, limit=LIMIT_ONCE)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] {e}\")\n",
    "            time.sleep(5)\n",
    "        time.sleep(POLL_SECONDS)\n",
    "\n",
    "else:  # MODE == \"once\"\n",
    "    for iv in INTERVALS:\n",
    "        for sym in SYMBOLS:\n",
    "            poll_once(sym, iv, limit=LIMIT_ONCE)\n",
    "    dbutils.notebook.exit(\"once done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e09747a4-f79f-43ea-b535-1aa252822fc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5651108161707683,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01a_ingest_bronze_binance.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
