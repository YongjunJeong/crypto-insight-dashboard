{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "226320c0-746b-4d91-b786-320c563befd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Notebook: Bronze ingest from Binance Kline REST (4h only)\n",
    "import time, json, random, datetime as dt, hashlib, requests\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "\n",
    "spark.conf.set(\"spark.databricks.delta.optimizeWrite\",\"true\")\n",
    "spark.conf.set(\"spark.databricks.delta.autoCompact\",\"true\")\n",
    "spark.sql(\"SET spark.sql.session.timeZone=UTC\")  # 경계 정합\n",
    "\n",
    "# ============= 실행/프로젝트 설정 =============\n",
    "MODE           = \"once\"                       # once | poll | forever | backfill | poll\n",
    "SYMBOLS        = [\"BTCUSDT\",\"ETHUSDT\",\"SOLUSDT\"]\n",
    "INTERVALS      = [\"4h\"]                        # 핵심만 수집\n",
    "LIMIT_ONCE     = 1000\n",
    "POLL_SECONDS   = 60\n",
    "MAX_POLLS      = 10\n",
    "BACKFILL_DAYS  = 200                           # MA200 안정화용 여유\n",
    "\n",
    "CATALOG = \"demo_catalog\"\n",
    "SCHEMA  = \"demo_schema\"\n",
    "TABLE        = f\"{CATALOG}.{SCHEMA}.bronze_charts\"        # raw append\n",
    "STATE_TABLE  = f\"{CATALOG}.{SCHEMA}.bronze_ingest_state\"  # 마지막 open_time(ms)\n",
    "\n",
    "BASE_URL = \"https://api.binance.com\"\n",
    "KLINES   = \"/api/v3/klines\"\n",
    "LIMIT_DEFAULT = 1000\n",
    "\n",
    "# ============= 테이블 준비 =============\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG}\")\n",
    "spark.sql(f\"CREATE SCHEMA  IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {TABLE} (\n",
    "  source            STRING,\n",
    "  symbol            STRING,\n",
    "  interval          STRING,\n",
    "  event_time        TIMESTAMP,  -- open_time(UTC)\n",
    "  ingest_time       TIMESTAMP,  -- 적재 시각(UTC)\n",
    "  unique_key        STRING,     -- \"symbol|interval|open_time(ms)\"\n",
    "  raw_json          STRING,     -- 원본 배열 JSON 문자열\n",
    "  api_endpoint      STRING,     -- 호출한 REST endpoint\n",
    "  api_params_hash   STRING,     -- 요청 파라미터 해시\n",
    "  dt                DATE        -- 파티션\n",
    ") USING DELTA\n",
    "PARTITIONED BY (dt)\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {STATE_TABLE} (\n",
    "  symbol     STRING,\n",
    "  interval   STRING,\n",
    "  last_open_time_ms LONG,\n",
    "  updated_at TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# ============= 유틸 =============\n",
    "def _params_hash(params: Dict) -> str:\n",
    "    raw = json.dumps(params, sort_keys=True, separators=(\",\",\":\"))\n",
    "    return hashlib.sha256(raw.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def _to_ms(ts: dt.datetime) -> int:\n",
    "    if ts.tzinfo is None:\n",
    "        ts = ts.replace(tzinfo=dt.timezone.utc)\n",
    "    return int(ts.timestamp() * 1000)\n",
    "\n",
    "def _from_ms(ms: int) -> dt.datetime:\n",
    "    return dt.datetime.fromtimestamp(ms/1000, tz=dt.timezone.utc)\n",
    "\n",
    "def binance_klines(symbol: str, interval: str, start_ms: Optional[int]=None,\n",
    "                   end_ms: Optional[int]=None, limit: int=LIMIT_DEFAULT,\n",
    "                   max_retries: int=5) -> Tuple[List[list], Dict[str,str]]:\n",
    "    url = BASE_URL + KLINES\n",
    "    q = {\"symbol\": symbol, \"interval\": interval, \"limit\": limit}\n",
    "    if start_ms is not None: q[\"startTime\"] = start_ms\n",
    "    if end_ms   is not None: q[\"endTime\"]   = end_ms\n",
    "\n",
    "    delay, last_headers = 1, {}\n",
    "    for _ in range(max_retries):\n",
    "        r = requests.get(url, params=q, timeout=30)\n",
    "        last_headers = {k:v for k,v in r.headers.items()}\n",
    "        if r.status_code == 200:\n",
    "            return r.json(), last_headers\n",
    "        if r.status_code == 429:\n",
    "            time.sleep(delay + random.uniform(0, 0.3))\n",
    "            delay = min(delay * 2, 16); continue\n",
    "        time.sleep(1 + random.uniform(0, 0.3))\n",
    "    r.raise_for_status()\n",
    "    return [], last_headers\n",
    "\n",
    "def _get_last_state(symbol: str, interval: str) -> Optional[int]:\n",
    "    df = spark.sql(f\"\"\"\n",
    "      SELECT last_open_time_ms\n",
    "      FROM {STATE_TABLE}\n",
    "      WHERE symbol = '{symbol}' AND interval = '{interval}'\n",
    "      ORDER BY updated_at DESC\n",
    "      LIMIT 1\n",
    "    \"\"\")\n",
    "    rows = df.collect()\n",
    "    return rows[0][0] if rows else None\n",
    "\n",
    "def _upsert_state(symbol: str, interval: str, last_ms: int):\n",
    "    now = dt.datetime.now(dt.timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    spark.sql(f\"\"\"\n",
    "      MERGE INTO {STATE_TABLE} t\n",
    "      USING (SELECT '{symbol}' AS symbol, '{interval}' AS interval,\n",
    "                    {last_ms} AS last_open_time_ms, TIMESTAMP('{now}') AS updated_at) s\n",
    "      ON t.symbol = s.symbol AND t.interval = s.interval\n",
    "      WHEN MATCHED THEN UPDATE SET last_open_time_ms = s.last_open_time_ms, updated_at = s.updated_at\n",
    "      WHEN NOT MATCHED THEN INSERT (symbol, interval, last_open_time_ms, updated_at)\n",
    "      VALUES (s.symbol, s.interval, s.last_open_time_ms, s.updated_at)\n",
    "    \"\"\")\n",
    "\n",
    "def _append_to_bronze(symbol: str, interval: str, rows: List[list], endpoint: str, params: Dict):\n",
    "    if not rows: return 0, None\n",
    "    param_hash = _params_hash(params)\n",
    "    now = dt.datetime.now(dt.timezone.utc)\n",
    "    now_s = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    recs, max_open_ms = [], None\n",
    "    for item in rows:\n",
    "        open_ms = int(item[0])\n",
    "        event_time = _from_ms(open_ms)\n",
    "        unique = f\"{symbol}|{interval}|{open_ms}\"\n",
    "        recs.append({\n",
    "          \"source\": \"binance.spot.klines\",\n",
    "          \"symbol\": symbol,\n",
    "          \"interval\": interval,\n",
    "          \"event_time\": event_time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "          \"ingest_time\": now_s,\n",
    "          \"unique_key\": unique,\n",
    "          \"raw_json\": json.dumps(item, separators=(\",\",\":\")),\n",
    "          \"api_endpoint\": endpoint,\n",
    "          \"api_params_hash\": param_hash,\n",
    "          \"dt\": event_time.date().isoformat()\n",
    "        })\n",
    "        if (max_open_ms is None) or (open_ms > max_open_ms):\n",
    "            max_open_ms = open_ms\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"source\",           StringType(), True),\n",
    "        StructField(\"symbol\",           StringType(), True),\n",
    "        StructField(\"interval\",         StringType(), True),\n",
    "        StructField(\"event_time\",       StringType(), True),\n",
    "        StructField(\"ingest_time\",      StringType(), True),\n",
    "        StructField(\"unique_key\",       StringType(), True),\n",
    "        StructField(\"raw_json\",         StringType(), True),\n",
    "        StructField(\"api_endpoint\",     StringType(), True),\n",
    "        StructField(\"api_params_hash\",  StringType(), True),\n",
    "        StructField(\"dt\",               StringType(), True),\n",
    "    ])\n",
    "    df = (spark.createDataFrame([Row(**r) for r in recs], schema)\n",
    "            .withColumn(\"event_time\",  to_timestamp(col(\"event_time\")))\n",
    "            .withColumn(\"ingest_time\", to_timestamp(col(\"ingest_time\")))\n",
    "            .withColumn(\"dt\",          col(\"dt\").cast(\"date\"))\n",
    "            .dropDuplicates([\"unique_key\"])\n",
    "            .repartition(\"dt\"))\n",
    "\n",
    "    count = df.count()\n",
    "    df.writeTo(TABLE).append()\n",
    "    return count, max_open_ms\n",
    "\n",
    "# ============= 모드별 동작 =============\n",
    "def backfill_symbol(symbol: str, interval: str, days: int = BACKFILL_DAYS, limit: int = LIMIT_DEFAULT):\n",
    "    now_utc   = dt.datetime.now(dt.timezone.utc)\n",
    "    start_utc = now_utc - dt.timedelta(days=days)\n",
    "    last_ms = _get_last_state(symbol, interval)\n",
    "    if last_ms: start_utc = _from_ms(last_ms) + dt.timedelta(milliseconds=1)\n",
    "\n",
    "    start_ms, end_ms = _to_ms(start_utc), _to_ms(now_utc)\n",
    "    total = 0; cursor_ms = start_ms\n",
    "    print(f\"[BACKFILL] {symbol} {interval} {_from_ms(start_ms)} → {_from_ms(end_ms)}\")\n",
    "\n",
    "    # 4h는 창을 넉넉히(예: 15일치) 잡아도 호출 수가 적음\n",
    "    step_ms = 1000 * 60 * 60 * 24 * 15\n",
    "    while cursor_ms < end_ms:\n",
    "        batch_end = min(end_ms, cursor_ms + step_ms)\n",
    "        params = {\"symbol\": symbol, \"interval\": interval, \"startTime\": cursor_ms, \"endTime\": batch_end, \"limit\": limit}\n",
    "        rows, headers = binance_klines(symbol, interval, start_ms=cursor_ms, end_ms=batch_end, limit=limit)\n",
    "        cnt, max_open_ms = _append_to_bronze(symbol, interval, rows, KLINES, params)\n",
    "        total += cnt\n",
    "        if max_open_ms is None: cursor_ms = batch_end + 1\n",
    "        else:\n",
    "            cursor_ms = max_open_ms + 1\n",
    "            _upsert_state(symbol, interval, max_open_ms)\n",
    "        time.sleep(0.2)\n",
    "    print(f\"[BACKFILL DONE] {symbol} {interval}: {total} rows\")\n",
    "\n",
    "def poll_once(symbol: str, interval: str, limit: int = 500):\n",
    "    params = {\"symbol\": symbol, \"interval\": interval, \"limit\": limit}\n",
    "    rows, headers = binance_klines(symbol, interval, limit=limit)\n",
    "    cnt, max_open_ms = _append_to_bronze(symbol, interval, rows, KLINES, params)\n",
    "    if max_open_ms is not None:\n",
    "        _upsert_state(symbol, interval, max_open_ms)\n",
    "    used_weight = headers.get(\"X-MBX-USED-WEIGHT-1m\") or headers.get(\"X-MBX-USED-WEIGHT\")\n",
    "    print(f\"[POLL] {symbol} {interval}: +{cnt} rows, last_open_ms={max_open_ms}, used_weight={used_weight}\")\n",
    "\n",
    "# MAIN\n",
    "if MODE == \"backfill\":\n",
    "    for sym in SYMBOLS:\n",
    "        backfill_symbol(sym, \"4h\", days=BACKFILL_DAYS, limit=LIMIT_DEFAULT)\n",
    "    dbutils.notebook.exit(\"backfill done\")\n",
    "elif MODE == \"poll\":\n",
    "    for i in range(MAX_POLLS):\n",
    "        for sym in SYMBOLS:\n",
    "            poll_once(sym, \"4h\", limit=LIMIT_ONCE)\n",
    "        time.sleep(POLL_SECONDS)\n",
    "    dbutils.notebook.exit(\"poll done\")\n",
    "elif MODE == \"forever\":\n",
    "    print(f\"[LIVE] start polling every {POLL_SECONDS}s\")\n",
    "    while True:\n",
    "        try:\n",
    "            for sym in SYMBOLS:\n",
    "                poll_once(sym, \"4h\", limit=LIMIT_ONCE)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] {e}\"); time.sleep(5)\n",
    "        time.sleep(POLL_SECONDS)\n",
    "else:  # once\n",
    "    for sym in SYMBOLS:\n",
    "        poll_once(sym, \"4h\", limit=LIMIT_ONCE)\n",
    "    dbutils.notebook.exit(\"once done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e09747a4-f79f-43ea-b535-1aa252822fc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5651108161707683,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01a_ingest_bronze_binance.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
